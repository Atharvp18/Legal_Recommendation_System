{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rgcksA9nITN"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UF7fWV23a7mt",
        "outputId": "83968959-a023-4ff7-c1c9-e29854c7c6a6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "DYMBCZ4xbnpD",
        "outputId": "27c512d4-9f94-4a5b-941f-5d782ebae0ef"
      },
      "outputs": [],
      "source": [
        "columns_to_read=['DocID','Case_Name', 'Judgment_Date','Author','Bench', 'CaseID/CitationID','Verdict']\n",
        "data = pd.read_csv(\"/content/drive/My Drive/legal_data.csv\",encoding='latin-1', usecols=columns_to_read)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QDrms2K9jxq"
      },
      "outputs": [],
      "source": [
        "# Code to get Statutes\n",
        "import math\n",
        "\n",
        "# Regular Expressions\n",
        "pattern_s = r'\\b[Ss]\\. (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_ss = r'\\b[Ss]s\\. (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_section = r'\\b[Ss]ection (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_sections =r'\\bsections (\\d+[A-Za-z]*(?:\\s*,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_subsection = r'\\b[Ss]ubsection (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_r = r'\\b[Rr]\\. (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "pattern_rule = r'\\b[Rr]ule (\\d+[A-Za-z]*(?:,\\s*\\d+[A-Za-z]*)*)(?:\\s+\\w+)*\\s+(IPC|Indian Penal Code|CRPC|Criminal Procedure Code|Code of Criminal Procedure|CPC|Code of Civil Procedure|CrPC)'\n",
        "\n",
        "# Extract section numbers and types for each row in the DataFrame\n",
        "for index, row in data.iterrows():\n",
        "    ipc_sections = set()\n",
        "    crpc_sections = set()\n",
        "    cpc_sections = set()\n",
        "    if math.isnan(row['DocID']):\n",
        "      continue\n",
        "    caseid = int(row['DocID'])\n",
        "    try:\n",
        "      matches_statutes = list()\n",
        "      extracted_sections = list()\n",
        "      with open(f\"/content/drive/My Drive/text_files/{caseid}.txt\", 'r') as file:\n",
        "        content = str(file.read())\n",
        "        matches_s = re.findall(pattern_s, content)\n",
        "        matches_statutes.append(matches_s)\n",
        "\n",
        "        matches_ss = re.findall(pattern_ss, content)\n",
        "        matches_statutes.append(matches_ss)\n",
        "\n",
        "        matches_section = re.findall(pattern_section, content)\n",
        "        matches_statutes.append(matches_section)\n",
        "\n",
        "        matches_sections = re.findall(pattern_sections, content)\n",
        "        matches_statutes.append(matches_sections)\n",
        "\n",
        "        matches_subsection = re.findall(pattern_subsection, content)\n",
        "        matches_statutes.append(matches_subsection)\n",
        "\n",
        "        matches_r = re.findall(pattern_r, content)\n",
        "        matches_statutes.append(matches_r)\n",
        "\n",
        "        matches_rule = re.findall(pattern_rule, content)\n",
        "        matches_statutes.append(matches_rule)\n",
        "        matches_statutes = [item for sublist in matches_statutes for item in sublist]\n",
        "\n",
        "        for match in matches_statutes:\n",
        "            if len(match) == 0:\n",
        "              continue\n",
        "            section_number = match[0]\n",
        "            section_type = match[1].strip().lower()\n",
        "            if section_type in ['ipc', 'indian penal code']:\n",
        "                ipc_sections.add(section_number)\n",
        "            elif section_type in ['crpc','criminal procedure code','code of criminal procedure']:\n",
        "                crpc_sections.add(section_number)\n",
        "            elif section_type in ['cpc','code of civil procedure']:\n",
        "                cpc_sections.add(section_number)\n",
        "            extracted_sections.append((section_type, section_number))\n",
        "        print(f\"Extracted Sections for Case {caseid}: {extracted_sections}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "      print(\"File not found\")\n",
        "\n",
        "    data.at[index,'IPC'] = str(list(ipc_sections))\n",
        "    data.at[index,'CRPC'] = str(list(crpc_sections))\n",
        "    data.at[index,'CPC'] = str(list(cpc_sections))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHAzVxFRScKi"
      },
      "outputs": [],
      "source": [
        "# Code to get Acts\n",
        "import math\n",
        "# Read the contents of the text file\n",
        "with open('/content/drive/My Drive/current_acts.txt', 'r') as file:\n",
        "    txt_contents = file.read().splitlines()\n",
        "\n",
        "# Compile regex patterns for efficient matching\n",
        "search_patterns = [re.compile(re.escape(line), re.IGNORECASE) for line in txt_contents]\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if math.isnan(row['DocID']):\n",
        "      continue\n",
        "  caseid = int(row['DocID'])\n",
        "  acts = []\n",
        "  # # Create a set to store unique matched parts\n",
        "  unique_matched_parts = set()\n",
        "  try:\n",
        "    columns_read=['Doc_ID','Sentence','Category']\n",
        "    df = pd.read_csv(f\"/content/drive/My Drive/Label_2RR/{caseid}.csv\",encoding='latin-1', usecols=columns_read)\n",
        "    df = df.dropna(how='all')\n",
        "    for idx, inner_row in df.iterrows():\n",
        "      sentence = str(inner_row['Sentence'])\n",
        "      #For extracting acts\n",
        "      if isinstance(sentence, str):\n",
        "         # Iterate through the search patterns and find matching parts\n",
        "         for pattern in search_patterns:\n",
        "           matches = pattern.findall(sentence)\n",
        "           unique_matched_parts.update(matches)\n",
        "\n",
        "      # Print the unique matched parts for the current file with the corresponding Doc_ID\n",
        "    if unique_matched_parts:\n",
        "      for matched_part in unique_matched_parts:\n",
        "        acts.append(matched_part)\n",
        "      print(caseid)\n",
        "      print(f\"{acts}\")\n",
        "\n",
        "    data.at[index, 'Acts'] = str(acts)\n",
        "\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(\"File not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qWQ43MJMAcyO",
        "outputId": "70c5b822-bf35-4026-9b0d-7f7429699b5d"
      },
      "outputs": [],
      "source": [
        "# Code to Append Acts which we extract from CITES in HTML Docs\n",
        "\n",
        "import math\n",
        "import ast\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if math.isnan(row['DocID']):\n",
        "        continue\n",
        "  case_id = int(row['DocID'])\n",
        "  print(case_id)\n",
        "  # Extract required information from <a> tags\n",
        "  try:\n",
        "      with open(f\"/content/drive/My Drive/input dataset/Cases/{case_id}.html\", 'r') as file:\n",
        "        content = str(file.read())\n",
        "        if 'Cites' in content and 'Citedby' in content:\n",
        "          parts1 = content.split('Cites', 1)\n",
        "          parts2 = parts1[1].split('Citedby', 1)\n",
        "          # Get the second part\n",
        "          result = parts2[0].strip()\n",
        "\n",
        "        elif 'Cites' in content:\n",
        "          parts1 = content.split('Cites', 1)\n",
        "          result = parts1[1]\n",
        "\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "        # Parse HTML using Beautiful Soup\n",
        "        soup = BeautifulSoup(result, 'html.parser')\n",
        "\n",
        "        # Find all div elements with class 'cite_title'\n",
        "        cite_titles = soup.find_all('div', class_='cite_title')\n",
        "\n",
        "        # Extract text inside the 'a' tag for each 'cite_title' div\n",
        "        titles = [title.find('a').get_text(strip=True) for title in cite_titles]\n",
        "        if pd.isna(row['Acts']):\n",
        "          data.at[index,'Acts'] = str(titles)\n",
        "        else:\n",
        "          act_list = ast.literal_eval(row['Acts'])\n",
        "          for title in titles:\n",
        "            if 'vs' in title or 'v.' in title or 'V.' in title or \"VS\" in title:\n",
        "              continue\n",
        "            act_list.append(title)\n",
        "\n",
        "          data.at[index,'Acts'] = str(act_list)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "      print(\"File not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DxTIv48ft7Ef",
        "outputId": "ddbe053f-3b16-4ede-d8a6-d202498f6f3a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "Doc_ids = list()\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if(math.isnan(row['DocID'])):\n",
        "    continue\n",
        "  Doc_ids.append(str(int(row['DocID'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XvTbKG64t7nx",
        "outputId": "82ed9558-73bf-4c9b-e979-d38c969bcfe0"
      },
      "outputs": [],
      "source": [
        "# Code to get Precedent and their DocIDs\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import math\n",
        "\n",
        "data['PrecedentCaseID'] = None\n",
        "data['Precedent'] = None\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if math.isnan(row['DocID']):\n",
        "        continue\n",
        "  case_id = int(row['DocID'])\n",
        "  print(case_id)\n",
        "  # Extract required information from <a> tags\n",
        "  results = []\n",
        "  unique_links = set()\n",
        "  check = True\n",
        "  try:\n",
        "    with open(f\"/content/drive/My Drive/input dataset/Cases/{case_id}.html\", 'r') as file:\n",
        "      content = str(file.read())\n",
        "      if 'BENCH' in content:\n",
        "        # Split the string based on the word 'JUDGMENT'\n",
        "        parts1 = content.split('BENCH', 1)\n",
        "        parts2 = parts1[1].split('JUDGMENT', 1)\n",
        "        # Get the second part\n",
        "        content = parts2[1].strip()\n",
        "\n",
        "      elif 'Bench' in content:\n",
        "        # Split the string based on the word 'JUDGMENT'\n",
        "        parts = content.split('Bench', 1)\n",
        "        # Get the second part\n",
        "        content = parts[1].strip()\n",
        "\n",
        "      else:\n",
        "        # Split the string based on the word 'JUDGMENT'\n",
        "        parts = content.split('JUDGMENT', 1)\n",
        "        # Get the second part\n",
        "        content = parts[1].strip()\n",
        "\n",
        "      # Parse the HTML content using BeautifulSoup\n",
        "      soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "      # Find all <a> tags with specific attributes\n",
        "      anchor_tags = soup.find_all('a', {'href': True})\n",
        "\n",
        "      for tag in anchor_tags:\n",
        "          link_href = tag['href']\n",
        "          link_text = tag.text.strip()\n",
        "          link_seperated = link_href.split('/')\n",
        "          link_href = link_seperated[-2]\n",
        "          if 'vs' in link_text or 'v.' in link_text:\n",
        "            if link_href in Doc_ids and link_href not in unique_links:\n",
        "              unique_links.add(link_href)\n",
        "              results.append((link_text, link_href))\n",
        "\n",
        "      # Print the extracted information\n",
        "      for result in results:\n",
        "        if check:\n",
        "          check = False\n",
        "          data.at[index, 'Precedent'] = result[0]\n",
        "          data.at[index, 'PrecedentCaseID'] = result[1]\n",
        "        else:\n",
        "          new_row = row.copy()\n",
        "          new_row['Precedent'] = result[0]\n",
        "          new_row['PrecedentCaseID'] = result[1]\n",
        "          # Append the new row to the DataFrame\n",
        "          data = data.append(new_row, ignore_index=True)\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(\"File not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9NTr5EjCR8zN",
        "outputId": "9a7c1873-5947-4655-8452-6e0afe097cdb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "docid_index_map = {}\n",
        "\n",
        "# Iterate through the DataFrame using iterrows\n",
        "for index, row in data.iterrows():\n",
        "    # Assuming 'DocID' is the column containing the DocID values\n",
        "    if math.isnan(row['DocID']):\n",
        "      continue\n",
        "    docid = int(row['DocID'])\n",
        "\n",
        "    # Store the index in the hashmap\n",
        "    docid_index_map[docid] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UkngNSdyrvLC",
        "outputId": "513d3aef-79fa-4611-ece4-4daf39aa91e0"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_character_ngrams(text, n):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    character_ngrams = set()\n",
        "    for token in tokens:\n",
        "        ngram_list = list(ngrams(token, n))\n",
        "        character_ngrams.update([\"\".join(ngram) for ngram in ngram_list])\n",
        "    return character_ngrams\n",
        "\n",
        "def jaccard_similarity(str1, str2, n=2):\n",
        "    ngrams1 = get_character_ngrams(str1, n)\n",
        "    ngrams2 = get_character_ngrams(str2, n)\n",
        "    intersection = len(ngrams1.intersection(ngrams2))\n",
        "    union = len(ngrams1) + len(ngrams2) - intersection\n",
        "    similarity_score = intersection / union\n",
        "    return similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1GqSVTESCqL"
      },
      "outputs": [],
      "source": [
        "# Code to get common acts using HTML files\n",
        "import math\n",
        "import ast\n",
        "\n",
        "data['CommonActs'] = None\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if math.isnan(row['DocID']):\n",
        "    continue\n",
        "  common_acts = set()\n",
        "  if row['PrecedentCaseID'] is None:\n",
        "    data.at[index, 'CommonActs'] = str(list(common_acts))\n",
        "    continue\n",
        "  if math.isnan(row['PrecedentCaseID']):\n",
        "    data.at[index, 'CommonActs'] = str(list(common_acts))\n",
        "    continue\n",
        "  print(int(row['DocID']))\n",
        "  current_row_acts = ast.literal_eval(row['Acts'])\n",
        "  precedent_row_acts_data = data.at[docid_index_map[int(row['PrecedentCaseID'])],'Acts']\n",
        "  if pd.isna(precedent_row_acts_data):\n",
        "    print(row['DocID'], \" iska skip hogya as nan acts in precedent\")\n",
        "    data.at[index, 'CommonActs'] = str(list(common_acts))\n",
        "    continue\n",
        "  precendent_row_acts = ast.literal_eval(precedent_row_acts_data)\n",
        "\n",
        "  for str1 in current_row_acts:\n",
        "    for str2 in precendent_row_acts:\n",
        "        similarity = jaccard_similarity(str1, str2)\n",
        "        if similarity >= 0.65:\n",
        "          if len(str1) > len(str2):\n",
        "            common_acts.add(str1)\n",
        "          else:\n",
        "            common_acts.add(str2)\n",
        "\n",
        "  data.at[index, 'CommonActs'] = str(list(common_acts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fONzAqB0m3oS",
        "outputId": "ddc26fa7-5958-41be-aa42-c8ad3be31f34"
      },
      "outputs": [],
      "source": [
        "# Code to create new columns 'CommonIPC', 'CommonCPC', and 'CommonCRPC'\n",
        "import math\n",
        "\n",
        "data['CommonIPC'] = None\n",
        "data['CommonCRPC'] = None\n",
        "data['CommonCPC'] = None\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if math.isnan(row['DocID']):\n",
        "    continue\n",
        "  common_ipc = list()\n",
        "  common_crpc = list()\n",
        "  common_cpc = list()\n",
        "  if math.isnan(row['PrecedentCaseID']):\n",
        "    data.at[index, 'CommonIPC'] = str(common_ipc)\n",
        "    data.at[index, 'CommonCRPC'] = str(common_crpc)\n",
        "    data.at[index, 'CommonCPC'] = str(common_cpc)\n",
        "    continue\n",
        "\n",
        "  print(int(row['DocID']))\n",
        "  current_ipc = set(str(data.at[index, 'IPC']).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "  current_crpc = set(str(data.at[index, 'CRPC']).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "  current_cpc = set(str(data.at[index, 'CPC']).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "\n",
        "  precedent_row_ipc_data = data.at[docid_index_map[int(row['PrecedentCaseID'])],'IPC']\n",
        "  precedent_row_crpc_data = data.at[docid_index_map[int(row['PrecedentCaseID'])],'CRPC']\n",
        "  precedent_row_cpc_data = data.at[docid_index_map[int(row['PrecedentCaseID'])],'CPC']\n",
        "\n",
        "  if pd.isna(precedent_row_ipc_data):\n",
        "    print(row['DocID'])\n",
        "    data.at[index, 'CommonIPC'] = str((common_ipc))\n",
        "\n",
        "  if pd.isna(precedent_row_crpc_data):\n",
        "    print(row['DocID'])\n",
        "    data.at[index, 'CommonCRPC'] = str((common_crpc))\n",
        "\n",
        "  if pd.isna(precedent_row_cpc_data):\n",
        "    print(row['DocID'])\n",
        "    data.at[index, 'CommonCPC'] = str((common_cpc))\n",
        "\n",
        "\n",
        "  precedent_ipc = set(str(precedent_row_ipc_data).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "  precedent_crpc = set(str(precedent_row_crpc_data).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "  precedent_cpc = set(str(precedent_row_cpc_data).replace('[','').replace(']','').replace('\\'','').split(','))\n",
        "\n",
        "\n",
        "  # Find common values for IPC, CPC, and CRPC\n",
        "  if data.at[index, 'CommonIPC'] != '[]':\n",
        "    common_ipc = list(precedent_ipc.intersection(current_ipc))\n",
        "    data.at[index, 'CommonIPC'] = common_ipc\n",
        "\n",
        "  if data.at[index, 'CommonCRPC'] != '[]':\n",
        "    common_crpc = list(precedent_crpc.intersection(current_crpc))\n",
        "    data.at[index, 'CommonCRPC'] = common_crpc\n",
        "\n",
        "  if data.at[index, 'CommonCPC'] != '[]':\n",
        "    common_cpc = list(precedent_cpc.intersection(current_cpc))\n",
        "    data.at[index, 'CommonCPC'] = common_cpc\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
